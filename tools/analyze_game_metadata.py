#!/usr/bin/env python3
"""
analyze_game_metadata.py - Analyze metadata coverage for game word filtering

Shows what metadata we have and what's missing for better filtering.
Helps identify gaps and improvement opportunities.
"""

import json
from pathlib import Path
from collections import Counter
from typing import Dict


def load_metadata(meta_path: Path) -> Dict[str, Dict]:
    """Load metadata JSON."""
    if not meta_path.exists():
        return {}

    with open(meta_path, 'r', encoding='utf-8') as f:
        metadata_list = json.load(f)

    return {entry['word']: entry for entry in metadata_list}


def analyze_coverage(metadata: Dict[str, Dict]):
    """Analyze what metadata fields we have."""
    total = len(metadata)

    # Count field coverage
    has_pos = sum(1 for e in metadata.values() if e.get('pos'))
    has_concreteness = sum(1 for e in metadata.values() if e.get('concreteness'))
    has_frequency = sum(1 for e in metadata.values() if e.get('frequency_tier'))
    has_labels = sum(1 for e in metadata.values() if e.get('labels'))
    has_gloss = sum(1 for e in metadata.values() if e.get('gloss'))

    # Count nouns
    nouns = [e for e in metadata.values() if 'noun' in e.get('pos', [])]
    concrete_nouns = [e for e in nouns if e.get('concreteness') == 'concrete']
    nouns_no_concrete = [e for e in nouns if not e.get('concreteness')]

    # Frequency distribution of nouns
    noun_freq = Counter()
    for noun in nouns:
        tier = noun.get('frequency_tier', 'rare')
        noun_freq[tier] += 1

    # Concreteness distribution
    concrete_dist = Counter()
    for entry in metadata.values():
        if 'noun' in entry.get('pos', []):
            concrete_dist[entry.get('concreteness', 'unknown')] += 1

    # Label coverage
    label_types = Counter()
    for entry in metadata.values():
        labels = entry.get('labels', {})
        for key in labels.keys():
            label_types[key] += 1

    return {
        'total': total,
        'has_pos': has_pos,
        'has_concreteness': has_concreteness,
        'has_frequency': has_frequency,
        'has_labels': has_labels,
        'has_gloss': has_gloss,
        'total_nouns': len(nouns),
        'concrete_nouns': len(concrete_nouns),
        'nouns_no_concrete': len(nouns_no_concrete),
        'noun_freq': noun_freq,
        'concrete_dist': concrete_dist,
        'label_types': label_types,
    }


def generate_report(distribution: str = 'core'):
    """Generate coverage analysis report."""
    meta_path = Path(f'data/build/{distribution}/{distribution}.meta.json')

    if not meta_path.exists():
        print(f"✗ Metadata not found: {meta_path}")
        print(f"  Run 'make build-{distribution}' first")
        return 1

    metadata = load_metadata(meta_path)
    analysis = analyze_coverage(metadata)

    report = f"# Game Metadata Coverage Analysis ({distribution.upper()})\n\n"
    report += "Generated by `tools/analyze_game_metadata.py`\n\n"
    report += "Analyzes metadata coverage for game word filtering.\n\n"
    report += "---\n\n"

    # Overall coverage
    report += "## Field Coverage\n\n"
    report += f"**Total entries:** {analysis['total']:,}\n\n"

    report += "| Field | Coverage | Percentage |\n"
    report += "|-------|------:|----------:|\n"

    fields = [
        ('POS tags', 'has_pos'),
        ('Concreteness', 'has_concreteness'),
        ('Frequency tier', 'has_frequency'),
        ('Labels', 'has_labels'),
        ('Gloss', 'has_gloss'),
    ]

    for field_name, key in fields:
        count = analysis[key]
        pct = count / analysis['total'] * 100
        report += f"| {field_name} | {count:,} | {pct:.1f}% |\n"

    report += "\n---\n\n"

    # Noun analysis
    report += "## Noun Analysis\n\n"
    report += f"**Total nouns:** {analysis['total_nouns']:,}\n"
    report += f"**Concrete nouns:** {analysis['concrete_nouns']:,}\n"
    report += f"**Nouns without concreteness data:** {analysis['nouns_no_concrete']:,}\n\n"

    if analysis['nouns_no_concrete'] > 0:
        pct_missing = analysis['nouns_no_concrete'] / analysis['total_nouns'] * 100
        report += f"⚠️  **{pct_missing:.1f}%** of nouns lack concreteness metadata!\n\n"

    # Concreteness distribution
    report += "### Concreteness Distribution\n\n"
    report += "| Type | Count |\n"
    report += "|------|------:|\n"
    for concrete_type, count in analysis['concrete_dist'].most_common():
        report += f"| {concrete_type} | {count:,} |\n"
    report += "\n"

    # Frequency distribution
    report += "### Frequency Distribution (Nouns Only)\n\n"
    report += "| Tier | Count |\n"
    report += "|------|------:|\n"
    for tier in ['top10', 'top100', 'top1k', 'top10k', 'top100k', 'rare']:
        count = analysis['noun_freq'].get(tier, 0)
        report += f"| {tier} | {count:,} |\n"
    report += "\n---\n\n"

    # Label types
    report += "## Label Types\n\n"
    report += "| Label Type | Usage Count |\n"
    report += "|-----------|------------:|\n"
    for label_type, count in analysis['label_types'].most_common():
        report += f"| {label_type} | {count:,} |\n"
    report += "\n---\n\n"

    # Recommendations
    report += "## Recommendations for Better Filtering\n\n"

    if analysis['nouns_no_concrete'] > 1000:
        report += "### 1. Improve Concreteness Detection\n\n"
        report += f"{analysis['nouns_no_concrete']:,} nouns lack concreteness data. Options:\n\n"
        report += "- **Heuristic-based:** Infer from word endings, domains, etc.\n"
        report += "- **ML-based:** Train classifier on known concrete/abstract words\n"
        report += "- **External data:** Import from concreteness databases\n"
        report += "- **Manual annotation:** Crowdsource or hire annotators\n\n"

    report += "### 2. Domain-based Filtering\n\n"
    report += "Use domain labels to exclude:\n"
    report += "- Adult content (sexuality, drugs, violence)\n"
    report += "- Jargon (technical, specialized)\n"
    report += "- Age-inappropriate (weapons, alcohol)\n\n"

    report += "### 3. Frequency-based Ranking\n\n"
    report += "Prioritize common words (top10k tier) that people know.\n\n"

    report += "### 4. Manual Review Process\n\n"
    report += "Even with good filters, manual review is essential:\n"
    report += "- Review top 500-1000 candidates\n"
    report += "- Create whitelist of verified words\n"
    report += "- Create blacklist of inappropriate words\n"
    report += "- Use lists to train better filters\n\n"

    # Write report
    output_path = Path(f'reports/game_metadata_analysis_{distribution}.md')
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"✓ Analysis report written to {output_path}")
    return output_path


if __name__ == '__main__':
    import sys

    distribution = sys.argv[1] if len(sys.argv) > 1 else 'core'

    if distribution not in ['core', 'plus']:
        print(f"Error: distribution must be 'core' or 'plus', got '{distribution}'")
        sys.exit(1)

    generate_report(distribution)
