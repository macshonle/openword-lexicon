#!/usr/bin/env python3
"""
inspect_pipeline.py - Inspect pipeline intermediate stages

Tracks how data transforms through the pipeline stages.
Shows counts, samples, and changes at each stage.
"""
import json
import random
from pathlib import Path
from typing import List, Dict, Any
from collections import Counter


def load_jsonl_sample(filepath: Path, n: int = 10) -> List[Dict[str, Any]]:
    """Load random sample of lines from JSONL file."""
    if not filepath.exists():
        return []

    with open(filepath, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    sample_size = min(n, len(lines))
    if sample_size == 0:
        return []

    sample_lines = random.sample(lines, sample_size)
    return [json.loads(line) for line in sample_lines]


def count_jsonl_lines(filepath: Path) -> int:
    """Count lines in JSONL file."""
    if not filepath.exists():
        return 0

    with open(filepath, 'r', encoding='utf-8') as f:
        return sum(1 for _ in f)


def analyze_labels(filepath: Path) -> Dict[str, Any]:
    """Analyze label distribution in a JSONL file."""
    if not filepath.exists():
        return {}

    pos_counts = Counter()
    register_counts = Counter()
    domain_counts = Counter()
    total = 0

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            entry = json.loads(line)
            total += 1

            labels = entry.get('labels', {})

            # Count POS tags
            for pos in labels.get('pos', []):
                pos_counts[pos] += 1

            # Count registers
            for reg in labels.get('register', []):
                register_counts[reg] += 1

            # Count domains
            for dom in labels.get('domain', []):
                domain_counts[dom] += 1

    return {
        'total_entries': total,
        'pos_distribution': dict(pos_counts.most_common(10)),
        'register_distribution': dict(register_counts.most_common(10)),
        'domain_distribution': dict(domain_counts.most_common(10))
    }


def inspect_stage(stage_name: str, filepath: Path, description: str) -> str:
    """Generate report section for a pipeline stage."""
    if not filepath.exists():
        return f"## {stage_name}\n\n⚠️ File not found: `{filepath}`\n\n"

    count = count_jsonl_lines(filepath)
    sample = load_jsonl_sample(filepath, 3)
    label_stats = analyze_labels(filepath)

    report = f"## {stage_name}\n\n"
    report += f"**Description:** {description}  \n"
    report += f"**Location:** `{filepath}`  \n"
    report += f"**Total entries:** {count:,}\n\n"

    # Label statistics
    if label_stats:
        report += "### Label Statistics\n\n"

        if label_stats.get('pos_distribution'):
            report += "**Top POS tags:**\n"
            for pos, count in label_stats['pos_distribution'].items():
                report += f"- `{pos}`: {count:,}\n"
            report += "\n"

        if label_stats.get('register_distribution'):
            report += "**Top registers:**\n"
            for reg, count in label_stats['register_distribution'].items():
                report += f"- `{reg}`: {count:,}\n"
            report += "\n"

        if label_stats.get('domain_distribution'):
            report += "**Top domains:**\n"
            for dom, count in label_stats['domain_distribution'].items():
                report += f"- `{dom}`: {count:,}\n"
            report += "\n"

    # Sample entries
    report += "### Sample Entries\n\n"
    for i, entry in enumerate(sample, 1):
        word = entry.get('word', 'N/A')
        freq_tier = entry.get('frequency_tier', 'N/A')
        sources = entry.get('sources', [])

        report += f"{i}. **`{word}`** (freq_tier: {freq_tier}, sources: {len(sources)})\n"
        report += "```json\n"
        report += json.dumps(entry, indent=2, ensure_ascii=False)
        report += "\n```\n\n"

    return report


def generate_report(distribution: str = 'core'):
    """Generate pipeline inspection report for a distribution."""
    random.seed(42)  # Reproducible samples

    report = f"# Pipeline Inspection Report ({distribution.upper()})\n\n"
    report += f"Generated by `tools/inspect_pipeline.py`\n\n"
    report += "This report shows how data transforms through each pipeline stage.\n\n"
    report += "---\n\n"

    # Define pipeline stages
    stages = []

    if distribution == 'core':
        stages = [
            ("After Core Ingest",
             Path(f'data/intermediate/{distribution}/01_core_ingest.jsonl'),
             "Initial ingestion from ENABLE and EOWL"),

            ("After WordNet Enrichment",
             Path(f'data/intermediate/{distribution}/02_wordnet_enrich.jsonl'),
             "Enriched with WordNet glosses and semantic relations"),

            ("After Frequency Tiers",
             Path(f'data/intermediate/{distribution}/03_frequency_tiers.jsonl'),
             "Assigned frequency tiers based on corpus data"),

            ("After Merge/Dedupe",
             Path(f'data/intermediate/{distribution}/04_merged.jsonl'),
             "Merged and deduplicated entries"),

            ("After Policy Filter",
             Path(f'data/filtered/{distribution}/filtered.jsonl'),
             "Filtered for family-friendly content"),
        ]
    else:  # plus
        stages = [
            ("After Core Ingest",
             Path(f'data/intermediate/{distribution}/01_core_ingest.jsonl'),
             "Initial ingestion from ENABLE and EOWL"),

            ("After Wiktionary Ingest",
             Path(f'data/intermediate/{distribution}/01b_wikt_ingest.jsonl'),
             "Additional entries from Wiktionary"),

            ("After WordNet Enrichment",
             Path(f'data/intermediate/{distribution}/02_wordnet_enrich.jsonl'),
             "Enriched with WordNet glosses and semantic relations"),

            ("After Frequency Tiers",
             Path(f'data/intermediate/{distribution}/03_frequency_tiers.jsonl'),
             "Assigned frequency tiers based on corpus data"),

            ("After Merge/Dedupe",
             Path(f'data/intermediate/{distribution}/04_merged.jsonl'),
             "Merged and deduplicated entries"),

            ("After Policy Filter",
             Path(f'data/filtered/{distribution}/filtered.jsonl'),
             "Filtered for family-friendly content"),
        ]

    for stage_name, filepath, description in stages:
        report += inspect_stage(stage_name, filepath, description)
        report += "\n---\n\n"

    # Summary table
    report += "## Stage Summary\n\n"
    report += "| Stage | Entries |\n"
    report += "|-------|--------:|\n"

    for stage_name, filepath, _ in stages:
        count = count_jsonl_lines(filepath)
        report += f"| {stage_name} | {count:,} |\n"

    # Write report
    output_path = Path(f'reports/pipeline_inspection_{distribution}.md')
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"✓ Pipeline inspection report ({distribution}) written to {output_path}")
    return output_path


if __name__ == '__main__':
    import sys

    distribution = sys.argv[1] if len(sys.argv) > 1 else 'core'

    if distribution not in ['core', 'plus']:
        print(f"Error: distribution must be 'core' or 'plus', got '{distribution}'")
        sys.exit(1)

    generate_report(distribution)
