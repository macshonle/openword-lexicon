#!/usr/bin/env python3
"""
inspect_trie.py - Inspect and test the final trie structure

Analyzes the MARISA trie and associated metadata.
Tests lookups, prefix searches, and data integrity.
"""
import json
import random
from pathlib import Path
import marisa_trie


def analyze_trie(trie_path: Path, meta_path: Path) -> dict:
    """Analyze trie structure and metadata."""
    if not trie_path.exists():
        return {'error': f'Trie not found at {trie_path}'}

    if not meta_path.exists():
        return {'error': f'Metadata not found at {meta_path}'}

    # Load trie
    trie = marisa_trie.Trie()
    trie.load(str(trie_path))

    # Load metadata (stored as array, convert to dict)
    with open(meta_path, 'r', encoding='utf-8') as f:
        metadata_list = json.load(f)

    # Convert list to dict keyed by word
    metadata = {entry['word']: entry for entry in metadata_list}

    # Get all words
    all_words = list(trie)

    # Calculate statistics
    word_count = len(all_words)
    trie_size_bytes = trie_path.stat().st_size
    meta_size_bytes = meta_path.stat().st_size
    bytes_per_word = trie_size_bytes / word_count if word_count > 0 else 0

    # Word length distribution
    lengths = [len(w) for w in all_words]
    min_len = min(lengths) if lengths else 0
    max_len = max(lengths) if lengths else 0
    avg_len = sum(lengths) / len(lengths) if lengths else 0

    # Find longest/shortest words
    shortest_words = sorted(all_words, key=len)[:5]
    longest_words = sorted(all_words, key=len, reverse=True)[:5]

    return {
        'word_count': word_count,
        'trie_size_bytes': trie_size_bytes,
        'meta_size_bytes': meta_size_bytes,
        'bytes_per_word': bytes_per_word,
        'min_word_length': min_len,
        'max_word_length': max_len,
        'avg_word_length': avg_len,
        'shortest_words': shortest_words,
        'longest_words': longest_words,
        'all_words': all_words,
        'metadata': metadata,
        'trie': trie
    }


def test_lookups(trie, all_words, metadata) -> str:
    """Test random word lookups."""
    report = "### Random Word Lookups\n\n"

    # Test 10 random words
    sample_words = random.sample(all_words, min(10, len(all_words)))

    for word in sample_words:
        # Check if in trie
        in_trie = word in trie

        # Get metadata
        meta = metadata.get(word, {})

        report += f"**`{word}`**  \n"
        report += f"- In trie: {in_trie}  \n"
        report += f"- Frequency tier: {meta.get('frequency_tier', 'N/A')}  \n"
        report += f"- Sources: {', '.join(meta.get('sources', []))}  \n"

        # Show labels if present
        labels = meta.get('labels', {})
        if labels.get('pos'):
            report += f"- POS: {', '.join(labels['pos'])}  \n"

        report += "\n"

    return report


def test_prefix_searches(trie, all_words) -> str:
    """Test prefix searches."""
    report = "### Prefix Search Tests\n\n"

    # Test some common prefixes
    test_prefixes = ['pre', 'un', 'anti', 'pro', 'sub']

    for prefix in test_prefixes:
        matches = trie.keys(prefix)
        count = len(matches)
        sample = matches[:5] if count > 5 else matches

        report += f"**Prefix: `{prefix}`**  \n"
        report += f"- Matches: {count:,}  \n"
        if sample:
            report += f"- Sample: {', '.join(f'`{w}`' for w in sample)}  \n"
        report += "\n"

    return report


def test_successor_predecessor(trie, all_words) -> str:
    """Test successor/predecessor lookups."""
    report = "### Successor/Predecessor Tests\n\n"

    # Test with a few random words
    sample_words = random.sample(all_words, min(5, len(all_words)))

    for word in sample_words:
        # Find neighbors
        try:
            idx = all_words.index(word)
            pred = all_words[idx - 1] if idx > 0 else None
            succ = all_words[idx + 1] if idx < len(all_words) - 1 else None

            report += f"**`{word}`**  \n"
            if pred:
                report += f"- Predecessor: `{pred}`  \n"
            if succ:
                report += f"- Successor: `{succ}`  \n"
            report += "\n"
        except ValueError:
            continue

    return report


def generate_report(distribution: str = 'core'):
    """Generate trie inspection report."""
    random.seed(42)  # Reproducible samples

    trie_path = Path(f'data/build/{distribution}/{distribution}.trie')
    meta_path = Path(f'data/build/{distribution}/{distribution}.meta.json')

    report = f"# Trie Inspection Report ({distribution.upper()})\n\n"
    report += f"Generated by `tools/inspect_trie.py`\n\n"
    report += "This report analyzes the final MARISA trie structure and metadata.\n\n"
    report += "---\n\n"

    # Analyze trie
    analysis = analyze_trie(trie_path, meta_path)

    if 'error' in analysis:
        report += f"## Error\n\n{analysis['error']}\n"
    else:
        # Basic statistics
        report += "## Basic Statistics\n\n"
        report += f"**Trie file:** `{trie_path}`  \n"
        report += f"**Metadata file:** `{meta_path}`  \n"
        report += f"**Total words:** {analysis['word_count']:,}  \n"
        report += f"**Trie size:** {analysis['trie_size_bytes']:,} bytes ({analysis['trie_size_bytes'] / 1024:.1f} KB)  \n"
        report += f"**Metadata size:** {analysis['meta_size_bytes']:,} bytes ({analysis['meta_size_bytes'] / 1024:.1f} KB)  \n"
        report += f"**Bytes per word (trie):** {analysis['bytes_per_word']:.2f}  \n"
        report += f"**Min word length:** {analysis['min_word_length']}  \n"
        report += f"**Max word length:** {analysis['max_word_length']}  \n"
        report += f"**Avg word length:** {analysis['avg_word_length']:.2f}  \n"
        report += "\n"

        # Extremes
        report += "## Word Length Extremes\n\n"
        report += f"**Shortest words:** {', '.join(f'`{w}`' for w in analysis['shortest_words'])}  \n"
        report += f"**Longest words:** {', '.join(f'`{w}`' for w in analysis['longest_words'])}  \n"
        report += "\n---\n\n"

        # Lookup tests
        report += "## Functionality Tests\n\n"
        report += test_lookups(analysis['trie'], analysis['all_words'], analysis['metadata'])
        report += "\n"
        report += test_prefix_searches(analysis['trie'], analysis['all_words'])
        report += "\n"
        report += test_successor_predecessor(analysis['trie'], analysis['all_words'])

    # Write report
    output_path = Path(f'reports/trie_inspection_{distribution}.md')
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ“ Trie inspection report ({distribution}) written to {output_path}")
    return output_path


if __name__ == '__main__':
    import sys

    distribution = sys.argv[1] if len(sys.argv) > 1 else 'core'

    if distribution not in ['core', 'plus']:
        print(f"Error: distribution must be 'core' or 'plus', got '{distribution}'")
        sys.exit(1)

    generate_report(distribution)
